{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb6ce55d",
   "metadata": {},
   "source": [
    "## Reading Diamond Prices\n",
    "This notebook will read in prices of diamonds from major websites, including: James Allen.\n",
    "\n",
    "It will create an output which is a Pandas Dataframe including prices and 4C related information. This will be written to a CSV.\n",
    "\n",
    "The Dataframe output will be passed on for Linear Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ccb88d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib3\n",
    "import re\n",
    "from datetime import datetime\n",
    "import requests\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f23178d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#General class and functions to check a robots.txt file. \n",
    " #Check the robots.txt file using this class. \n",
    " #robotlink is the URL of the robots file\n",
    " #urlcheck is a string containing the url you want to review. It is searched suing string contains. \n",
    "class Robots_Review:\n",
    "    def __init__(self,robotlink,urlcheck): # constructor method\n",
    "        self.link=robotlink\n",
    "        self.tocheck=urlcheck\n",
    "        robots_file = requests.get(robots)\n",
    "        code = robots_file.status_code\n",
    "        if code != 200:\n",
    "            sys.exit(f\"Error, {robotlink} returned a code {code} error!\")\n",
    "        if code == 200:\n",
    "            req = urllib3.PoolManager()\n",
    "            response = req.request('GET', robotlink) #Read the robots file.\n",
    "            soup = BeautifulSoup(response.data, 'html.parser')\n",
    "            lines = str(soup).splitlines() #Turns the HTML into a set of links. \n",
    "            #Now turn the lines into a readable dataframe.\n",
    "            dataf = pd.DataFrame()\n",
    "            for line in lines:\n",
    "                if len(line.split(\":\"))>1: #Check the line has the colon\n",
    "                    condition,link = line.split(\":\")[0],line.split(\":\")[1]\n",
    "                    dataf=dataf.append(pd.DataFrame(data=\n",
    "                        {\"Condition\":[condition],\"Subdirectory\":[link]        }\n",
    "                    ))\n",
    "            self.robotsdf = dataf\n",
    "    \n",
    "    #Check a specific subdirectory. This \n",
    "    def find_subdir(self):\n",
    "        mask = self.robotsdf['Subdirectory'].str.contains(self.tocheck)\n",
    "        #print(mask)\n",
    "        output = self.robotsdf[mask]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b001ef",
   "metadata": {},
   "source": [
    "### Reading from the James Allen website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69aaff3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Condition                                Subdirectory\n",
      "0     Allow   /loose-diamonds/round-cut/?Cut=TrueHearts\n",
      "0     Allow       /loose-diamonds/all-diamonds/?CM=True\n"
     ]
    }
   ],
   "source": [
    "#Check robots txt\n",
    "robots = 'https://www.jamesallen.com/robots.txt'\n",
    "\n",
    "classtest=Robots_Review(robots,'loose-diamonds')\n",
    "#print(classtest.robotsdf)\n",
    "print(classtest.find_subdir())\n",
    "#We are good to go searching loose-diamonds!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a2ef315",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now do the web scraping!\n",
    "#Create a function, so you can parametrize this more easily. \n",
    "\n",
    "# Fetch the html file\n",
    "caratsearches = np.arange(.2,7,.1) #You will search over all of these carat weights\n",
    "labgrown=True #Defines if it's lab grown\n",
    "pagesno = 40 #this is the number of pages you search for every time. \n",
    "write = True\n",
    "\n",
    "\n",
    "def scrape_JamesAllen(caratsearches=np.arange(.2,7,.1),labgrown=True,pages = 40,write = True):\n",
    "    #Some initiatilizations to start. \n",
    "    data=pd.DataFrame() #Define the output.\n",
    "    pagenolist=np.arange(0,pages) #The number of pages searched on every search query.\n",
    "    counter = 0 #Counter variables. \n",
    "    noter = 0\n",
    "    #Create the appropriate text for the labgrown file.\n",
    "    if labgrown == True: lgtext = 'LabGrown'\n",
    "    if labgrown == False: lgtext = 'Natural'\n",
    "    if write == True :  #Create the descriptive filename. \n",
    "        cmin = round(np.min(caratsearches),1)\n",
    "        cmax = round(np.max(caratsearches),1)\n",
    "        filenamer = f'jamesalleninv_{lgtext}_{cmin}-{cmax}_{datetime.now().year}{datetime.now().month}{datetime.now().day}.csv'\n",
    "        print(f'Initializing. I will write output to {filenamer}')\n",
    "    for caratwtlo in caratsearches:\n",
    "        carat_str=f'CaratFrom={str(caratwtlo)}&CaratTo={str(caratwtlo+.1)}' \n",
    "        for pageno in pagenolist:\n",
    "            url = f'https://www.jamesallen.com/loose-diamonds/all-diamonds/page-{pageno}/?Color=Y,J,I,H,G,F,E,D&Shape=all-diamonds&Clarity=I1,SI2,SI1,VS2,VS1,VVS2,VVS1,IF,FL&{carat_str}&IsLabDiamond={str(labgrown)}&resultsView=List'\n",
    "            #All of the page URLs follow the same format with the exception of one number followed by 'page-'\n",
    "\n",
    "            req = urllib3.PoolManager()\n",
    "            res = req.request('GET', url)\n",
    "            if res.status != 200: #Check it finds a webpage. \n",
    "                print(f\"Error, {url} returned a code {res.status} error!\")\n",
    "                break #Stops the loop the first time an entry is not found. \n",
    "            if res.status == 200:\n",
    "                soup = BeautifulSoup(res.data, 'html.parser')\n",
    "                body = soup.find(\"body\")\n",
    "                contents = soup.find_all(class_= 'product_pod')\n",
    "\n",
    "                #This defines the general serach text, that will find all values matching the general format \n",
    "                #div and galleryItem_1_DIGIT\n",
    "                #Use regex to do this general serach. \n",
    "                stringpat= f\"galleryItem_{pageno}\"+r\"_\\d\"\n",
    "                string=re.compile(stringpat)\n",
    "\n",
    "                #Define the list of terms to be searched.\n",
    "                search_list=[['Carat Weight','li-carat'],['Shape','li-shape'],['Color','li-color'],['Clarity','li-clarity'],['Cut','li-cut'],['Lab','li-lab']]\n",
    "                #Search for each entry for each diamond on the webpage.\n",
    "                for div in body.find_all(\"div\",{\"data-qa\": string}):\n",
    "                    #Then search over the names and fields given in search_list\n",
    "                    inter1=pd.DataFrame()\n",
    "                    for name,field in search_list:\n",
    "                        #Create the itemized output.\n",
    "                        inter = pd.DataFrame(data=\n",
    "                                            { name:[div.find(\"div\",{\"data-qa\": field}).text]   })\n",
    "                        #Then, build a column up from the single div.\n",
    "                        inter1 = pd.concat([inter1,inter],axis=1)\n",
    "                    #The serach for a price is slightly different, because it is in a span. So just add in the price.\n",
    "                    pricesearcher=div.find(\"span\",\"base-price--xVDZZ\").text\n",
    "                    #Then add it again.\n",
    "                    inter = pd.DataFrame(data= {\n",
    "                        \n",
    "                        \"Price (USD $)\":[pricesearcher.replace(\"$\", \"\")] ,\n",
    "                        \"Diamond Type\":[f\"{lgtext}\"],\n",
    "                        \"Timestamp\":[datetime.now()],\n",
    "                        \"Webpage\":[url.split(\"/\")[2]],\"URL\":[url]})\n",
    "                    inter1 = pd.concat([inter1,inter],axis=1)\n",
    "                    #Finally build the whole ouput\n",
    "                    data = data.append(inter1)\n",
    "                \n",
    "                counter = counter + 1 #Counting the number of iterations.\n",
    "                # adding 3 seconds time delay to avoid triggering a DNS block\n",
    "                time.sleep(3)\n",
    "                if counter > 30 :\n",
    "                    noter = noter+ counter\n",
    "                    print(f\"I have successfully read in {noter} webpages, with data for {len(data)} stones!\")\n",
    "                    counter = 0\n",
    "                    time.sleep (27) #Doing a 30 second wait before pulling again, to vaoid DNS blocks\n",
    "        print(f\"Output summary: {data.info()}\")\n",
    "        print(f\"output head: {data.head()}\")\n",
    "        if write == True: data.to_csv(f\"data\\{filenamer}\")\n",
    "        return data\n",
    "\n",
    "    print(data.info())\n",
    "    print(data.head())\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "53ccdd48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing. I will write output to jamesalleninv_LabGrown_0.2-0.3_2022429.csv\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 9 entries, 0 to 0\n",
      "Data columns (total 11 columns):\n",
      " #   Column         Non-Null Count  Dtype         \n",
      "---  ------         --------------  -----         \n",
      " 0   Carat Weight   9 non-null      object        \n",
      " 1   Shape          9 non-null      object        \n",
      " 2   Color          9 non-null      object        \n",
      " 3   Clarity        9 non-null      object        \n",
      " 4   Cut            9 non-null      object        \n",
      " 5   Lab            9 non-null      object        \n",
      " 6   Price (USD $)  9 non-null      object        \n",
      " 7   Diamond Type   9 non-null      object        \n",
      " 8   Timestamp      9 non-null      datetime64[ns]\n",
      " 9   Webpage        9 non-null      object        \n",
      " 10  URL            9 non-null      object        \n",
      "dtypes: datetime64[ns](1), object(10)\n",
      "memory usage: 864.0+ bytes\n",
      "Output summary: None\n",
      "output head:   Carat Weight  Shape   Color Clarity        Cut  Lab Price (USD $)  \\\n",
      "0         0.30   Pear  Yellow    VVS2          -  IGI           230   \n",
      "0         0.25  Round  Yellow     VS2          -  IGI           250   \n",
      "0         0.24  Round  Yellow     VS1          -  IGI           270   \n",
      "0         0.30  Round       G     VS1  Very Good  IGI           270   \n",
      "0         0.24  Round  Yellow     VS1          -  IGI           280   \n",
      "\n",
      "  Diamond Type                  Timestamp             Webpage  \\\n",
      "0     LabGrown 2022-04-29 17:11:54.289398  www.jamesallen.com   \n",
      "0     LabGrown 2022-04-29 17:11:54.294385  www.jamesallen.com   \n",
      "0     LabGrown 2022-04-29 17:11:54.299371  www.jamesallen.com   \n",
      "0     LabGrown 2022-04-29 17:11:54.304358  www.jamesallen.com   \n",
      "0     LabGrown 2022-04-29 17:11:54.308347  www.jamesallen.com   \n",
      "\n",
      "                                                 URL  \n",
      "0  https://www.jamesallen.com/loose-diamonds/all-...  \n",
      "0  https://www.jamesallen.com/loose-diamonds/all-...  \n",
      "0  https://www.jamesallen.com/loose-diamonds/all-...  \n",
      "0  https://www.jamesallen.com/loose-diamonds/all-...  \n",
      "0  https://www.jamesallen.com/loose-diamonds/all-...  \n"
     ]
    }
   ],
   "source": [
    "# Fetch the html file\n",
    "caratsearches = np.arange(.2,7,.1) #You will search over all of these carat weights\n",
    "labgrown=True #Defines if it's lab grown\n",
    "pagesno = 40 #this is the number of pages you search for every time. \n",
    "write = True\n",
    "\n",
    "\n",
    "t1=scrape_JamesAllen(np.arange(.2,.4,.1),labgrown=True,pages = 2,write = True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
